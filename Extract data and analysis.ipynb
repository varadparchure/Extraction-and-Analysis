{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b0d3441",
   "metadata": {},
   "source": [
    "## Extracting the Data from given input file for each link\n",
    "\n",
    "1. Have used beautifulsoup to parse HTML content\n",
    "2. Following code generates seperate txt files for each given url\n",
    "3. If URL not present throw error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b253468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Article text or title not found for URL_ID 44\n",
      "Error: Article text or title not found for URL_ID 51\n",
      "Error: Article text or title not found for URL_ID 57\n",
      "Error: Article text or title not found for URL_ID 91\n",
      "Error: Article text or title not found for URL_ID 92\n",
      "Error: Article text or title not found for URL_ID 100\n",
      "Error: Article text or title not found for URL_ID 107\n",
      "Error: Article text or title not found for URL_ID 108\n",
      "Error: Article text or title not found for URL_ID 112\n",
      "Error: Article text or title not found for URL_ID 144\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract article text and title and save them in a file\n",
    "def extract_article(url_id, url):\n",
    "    # Send a GET request to the URL and retrieve the HTML content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Use BeautifulSoup to parse the HTML content and extract the article text and title\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the article text and title based on the HTML tags and attributes\n",
    "    article_elem = soup.find(class_=\"td-post-content tagdiv-type\")\n",
    "    title_elem = soup.find('h1', class_='entry-title')\n",
    "\n",
    "    # Check if the article text and title were found on the page\n",
    "    if article_elem is not None and title_elem is not None:\n",
    "        article_text = article_elem.text.strip()\n",
    "        title = title_elem.text.strip()\n",
    "\n",
    "        # Save the article text and title in a text file with URL_ID as its filename\n",
    "        with open(f'{url_id}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(f'Title: {title}\\n\\n')\n",
    "            f.write(f'Article Text:\\n\\n{article_text}')\n",
    "    else:\n",
    "        print(f'Error: Article text or title not found for URL_ID {url_id}')\n",
    "\n",
    "# Read the input file into a pandas dataframe\n",
    "df = pd.read_excel('input.xlsx')\n",
    "\n",
    "# Loop over the rows of the dataframe and extract the article text and title for each URL\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    extract_article(url_id, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50334bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73311285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d40567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdff08b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5ac01d",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47be0862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2a28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a75fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef714a2a",
   "metadata": {},
   "source": [
    "## Analyse the extracted txt files\n",
    "\n",
    "1. imported stop word files and have set +ve and -ve words files\n",
    "2. Output file analysis.csv is generated with the mentioned headers\n",
    "3. The file name is compared to each indiviual file in generated txt folder (contains all the txt files)\n",
    "4. Loop through each file to generate the required variables using formule  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "663cc6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pyphen\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Set up stop words\n",
    "stop_words_files = ['StopWords_Generic.txt', 'StopWords_GenericLong.txt', 'StopWords_Names.txt','StopWords_Currencies.txt',\n",
    "                    'StopWords_Auditor.txt', 'StopWords_DatesandNumbers.txt']\n",
    "\n",
    "stop_words = set()\n",
    "for file in stop_words_files:\n",
    "    with open(file, 'r') as f:\n",
    "        words = f.read().splitlines()\n",
    "        stop_words.update(words)\n",
    "        \n",
    "# Set up positive and negative words\n",
    "positive_words = set()\n",
    "negative_words = set()\n",
    "with open('positive-words.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "    positive_words.update(words)\n",
    "with open('negative-words.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "    negative_words.update(words)\n",
    "        \n",
    "# Set up output CSV file\n",
    "output_file = 'analysis.csv'\n",
    "header = ['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', \"SUBJECTIVITY SCORE\", \"AVG SENTENCE LENGTH\",\n",
    "         \"PERCENTAGE OF COMPLEX WORDS\",\"COMPLEX WORD COUNT\",\"FOG INDEX\",\"AVG NUMBER OF WORDS PER SENTENCE\",\"WORD COUNT\",\n",
    "         \"AVG WORD LENGTH\",\"PERSONAL PRONOUNS\",\"SYLLABLE PER WORD\"]\n",
    "with open(output_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "        \n",
    "    # Loop through text files and perform analysis\n",
    "    for filename in os.listdir('C:/Users/modx/Desktop/generated_txt/'):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open('C:/Users/modx/Desktop/generated_txt/' + filename, 'r', encoding='utf-8') as f:\n",
    "                \n",
    "                text = f.read().lower()\n",
    "                \n",
    "                # Clean the text\n",
    "                words = word_tokenize(text)\n",
    "                words_cleaned = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "                \n",
    "                # Calculate positive and negative scores\n",
    "                positive_score = sum([1 for word in words_cleaned if word in positive_words])\n",
    "                negative_score = sum([1 for word in words_cleaned if word in negative_words])\n",
    "                \n",
    "                # Calculate polarity score\n",
    "                polarity_score = round((positive_score - negative_score) / ((positive_score + negative_score) + 0.000001),2)\n",
    "                \n",
    "                # calculate the subjectivity score\n",
    "                subjectivity_score = round(positive_score + negative_score / (len(words_cleaned) + 0.000001),2) \n",
    "                \n",
    "                #calculate the average sentence length\n",
    "                sentences = sent_tokenize(text)\n",
    "                total_sentences = len(sentences)\n",
    "                avg_sentence_length = round(len(words_cleaned) / len(sentences),2)\n",
    "                \n",
    "                #calculate the percentage of complex words\n",
    "                \n",
    "                cmplx_words = []\n",
    "                #use pyphen for syllabus count\n",
    "                dic = pyphen.Pyphen(lang='en')\n",
    "                for word in words_cleaned:\n",
    "                    \n",
    "                    syllable_count = len(dic.inserted(word).split('-'))\n",
    "       \n",
    "                    if syllable_count > 2:\n",
    "                        cmplx_words.append(word)\n",
    "\n",
    "                #Complex words \n",
    "                complex_word_count = len(cmplx_words)\n",
    "                \n",
    "                complex_words = (complex_word_count / len(words_cleaned))*100 \n",
    "\n",
    "                # convert to percentage\n",
    "                percent_complex_words = str(complex_words) + \"%\" \n",
    "                \n",
    "                \n",
    "                # Calculate FOG Index\n",
    "                fog_index = round(0.4 * (avg_sentence_length + complex_words),2)\n",
    "                \n",
    "                # Calculate Average Number of Words per Sentence and Syllables per Word\n",
    "                avg_words_per_sentence = round(len(words_cleaned) / len(sentences),2)\n",
    "                \n",
    "               \n",
    "                # Count the total number of cleaned words\n",
    "                word_count = len(words_cleaned)\n",
    "                \n",
    "                # avg word length\n",
    "                total_chars = sum(len(word) for word in words_cleaned)\n",
    "                avg_word_length = round(total_chars / len(words_cleaned),2)\n",
    "                \n",
    "                #Avg words / sentence\n",
    "                avg_words_per_sentence = round(word_count / total_sentences,2)\n",
    "           \n",
    "                #pronoun \n",
    "                personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
    "                counts = {}\n",
    "                for pronoun in personal_pronouns:\n",
    "                    pattern = r\"\\b\" + pronoun + r\"\\b\"\n",
    "                    count = len(re.findall(pattern, text, re.IGNORECASE))\n",
    "                    counts[pronoun] = count\n",
    "                    \n",
    "                pronoun_count = sum(counts.values())\n",
    "                \n",
    "                #syllabels per word\n",
    "                #use pyphen for syllabus count\n",
    "                dic = pyphen.Pyphen(lang='en')\n",
    "                for word in words_cleaned:\n",
    "                    \n",
    "                    syllable_count = len(dic.inserted(word).split('-'))\n",
    "        \n",
    "                # Write to CSV\n",
    "                url_id = filename.split('.')[0]\n",
    "                row = [url_id, positive_score, negative_score, polarity_score, subjectivity_score,avg_sentence_length,\n",
    "                       percent_complex_words,complex_word_count,fog_index,avg_words_per_sentence,word_count, avg_word_length,\n",
    "                      pronoun_count,syllable_count]\n",
    "                \n",
    "                \n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91357cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84647cc8",
   "metadata": {},
   "source": [
    "## Sort the rows wrt to URL as required in the output file and merge with the input file to get the required output\n",
    "\n",
    "my machine kept arranging the url in some weird fashion, so the sorting according to the url step was added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "c9dd4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the rows\n",
    "\n",
    "import csv\n",
    "\n",
    "filename = \"analysis.csv\"\n",
    "rows = []\n",
    "\n",
    "# Read the CSV file and skip the header row\n",
    "with open(filename, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)  # skip the header row\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "# Sort the rows by the first column (as an integer)\n",
    "rows.sort(key=lambda x: int(x[0]))\n",
    "\n",
    "# Write the sorted rows back to the same file\n",
    "with open(filename, 'w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "33045bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e2cda1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# read the two CSV files\n",
    "df1 = pd.read_csv('analysis.csv')\n",
    "df2 = pd.read_excel('Input.xlsx')\n",
    "\n",
    "# merge the two dataframes on the 'URL_ID' column\n",
    "merged_df = pd.merge(df1, df2, on='URL_ID', how='left')\n",
    "\n",
    "# reorder the columns\n",
    "merged_df = merged_df[[\"URL_ID\",\"URL\", \"POSITIVE SCORE\" ,\"NEGATIVE SCORE\",\"POLARITY SCORE\",\"SUBJECTIVITY SCORE\",\"AVG SENTENCE LENGTH\",\n",
    "                       \"PERCENTAGE OF COMPLEX WORDS\" ,\"FOG INDEX\", \"AVG NUMBER OF WORDS PER SENTENCE\" ,\"COMPLEX WORD COUNT\",\n",
    "                       \"WORD COUNT\", \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"]]\n",
    "\n",
    "# save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7699360f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c290f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
